{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the scores produced by every method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from utils import POEM_FOLDER\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def get_poem_dataset_lookup():\n",
    "    \"\"\"\n",
    "    Return a dictionary with poems as key and their respective dataset association as value\n",
    "    \"\"\"\n",
    "    poem_dataset_lookup = {}\n",
    "    with open(join(POEM_FOLDER, \"consolidated_batches.csv\"), \"r\") as batch_file:\n",
    "        csv_reader = csv.reader(batch_file)\n",
    "        for row in csv_reader:\n",
    "            poem1 = row[1]\n",
    "            poem2 = row[2]\n",
    "            dataset1 = row[3]\n",
    "            dataset2 = row[4]\n",
    "            \n",
    "            poem_dataset_lookup[poem1] = dataset1\n",
    "            poem_dataset_lookup[poem2] = dataset2\n",
    "    return poem_dataset_lookup\n",
    "        \n",
    "\n",
    "def get_scores(method, subset=True):\n",
    "    \"\"\"\n",
    "    Retrieves the score generated from a specific method\n",
    "    \"\"\"\n",
    "    poem_scores = {}\n",
    "    poem_dataset_lookup = get_poem_dataset_lookup()\n",
    "    subset_filename = \"subset\" if subset else \"\"\n",
    "    filename = \"scores/\" + method + f\"_{subset_filename}.csv\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        header = next(csv_reader)\n",
    "        for row in csv_reader:\n",
    "            poem = row[0]\n",
    "            dataset = poem_dataset_lookup[poem]\n",
    "            scores = {\"poem\": poem, \"dataset\": dataset}\n",
    "            for idx, score in enumerate(row[1:], 1):\n",
    "                cat = method + \"_\" + header[idx]\n",
    "                scores[cat] = score\n",
    "            poem_scores[poem] = scores \n",
    "    return poem_scores, header\n",
    "                \n",
    "method_scores = []\n",
    "subset = True\n",
    "for method in ['bertranker', 'crowdgppl', 'bws', 'gppl']:\n",
    "    method_scores.append(get_scores(method, subset)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all dictionaries into one on poem basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "cons_dicts = {}\n",
    "for poem in method_scores[0]:\n",
    "    cons_dicts[poem] = method_scores[0][poem]\n",
    "    for d in method_scores[1:]:\n",
    "        current_scores = cons_dicts[poem]\n",
    "        other_scores = d[poem]\n",
    "        current_scores = {**current_scores, **other_scores}\n",
    "        cons_dicts[poem] = current_scores        \n",
    "        \n",
    "print(len(cons_dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write all score into one large file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Get any poem (in this case the first) to retrieve the colomn names\n",
    "first_key = list(cons_dicts.keys())[0]\n",
    "# Retrieve all fieldnames in under one poem entry\n",
    "fieldnames = list(cons_dicts[first_key].keys())\n",
    "\n",
    "with open(\"scores/consolidated_single_poems.csv\", \"w+\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fieldnames)\n",
    "    # Iterate over poems in the merged dictionary\n",
    "    for p in cons_dicts:\n",
    "        line = []\n",
    "        # Collect all data for that poem\n",
    "        for col in fieldnames:\n",
    "            try:\n",
    "                value = cons_dicts[p][col]\n",
    "                line.append(value)\n",
    "            except:\n",
    "                print(col)\n",
    "                print(p)\n",
    "        writer.writerow(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Scores previously collected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math \n",
    "# Initialize mins and max for each of the 44 columns\n",
    "mins = [math.inf]*44\n",
    "maxs = [-math.inf]*44\n",
    "\n",
    "lines = []\n",
    "with open(\"scores/consolidated_single_poems.csv\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    header = next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        lines.append(row)\n",
    "        for i, num in enumerate(row[2:]):\n",
    "            mins[i] = min(mins[i], float(num))\n",
    "            maxs[i] = max(maxs[i], float(num))\n",
    "\n",
    "\n",
    "normalized_lines = []\n",
    "for line in lines:\n",
    "    norm_line = line[:2]\n",
    "    for i, num in enumerate(line[2:]):\n",
    "        norm_val = (float(num) - mins[i])/(maxs[i] - mins[i])\n",
    "        norm_line.append(norm_val)\n",
    "    normalized_lines.append(norm_line)\n",
    "\n",
    "with open(\"scores/normalized_scores.csv\", \"w+\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(header)\n",
    "    for line in normalized_lines:\n",
    "        csv_writer.writerow(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_lookup = get_poem_dataset_lookup()\n",
    "datasets = set(list(dataset_lookup.values()))\n",
    "\n",
    "dataset_samples = {}\n",
    "for ds in datasets:\n",
    "    print(ds)\n",
    "    # Get 10 samples for each ds\n",
    "    samples = []\n",
    "    for poem, dataset in dataset_lookup.items():\n",
    "        if dataset == ds:\n",
    "            p = poem.replace(\"<br>\", \"\\n\")\n",
    "            samples.append(p)\n",
    "        if len(samples) >= 10:\n",
    "            break\n",
    "    dataset_samples[ds] = samples\n",
    "\n",
    "for key in dataset_samples:\n",
    "    print(f\"~~~~~~~~~~~ {key} ~~~~~~~~~~~\")\n",
    "    for sample in dataset_samples[key]:\n",
    "        print(\"=========\")\n",
    "        print(sample)\n",
    "        print(\"=========\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
